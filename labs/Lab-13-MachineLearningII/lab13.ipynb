{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 13: Machine Learning II\n",
    "\n",
    "Please complete this lab by providing answers in cells after the question. Use **Code** cells to write and run any code you need to answer the question and **Markdown** cells to write out answers in words. After you are finished with the assignment, remember to download it as an **HTML file** and submit it in **ELMS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datascience import *\n",
    "\n",
    "# These lines do some fancy plotting magic.\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will go over using decision tree models. We'll then include them in machine learning workflow to find the best models possible. Remember, the machine learning workflow we will use is as follows:\n",
    "- Create train and test sets\n",
    "- Fit the models using train set\n",
    "- Predict using the test set\n",
    "- Evaluate models using metrics such as precision and recall\n",
    "- Make your conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulse of the Nation - Climate Change\n",
    "\n",
    "Suppose we want to predict which people do not believe in climate change or believe it is not caused by people. To build our model, we will use the Cards Against Humanity Pulse of the Nation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cah = Table.read_table(\"201709-CAH_PulseOfTheNation.csv\")\n",
    "cah.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have a decent amount of variables that are strings in this dataset. We can't use the strings as is with the `sklearn` package, so we need to make sure to change those in to numerical or boolean values first. To do this, we'll create what are called **dummy variables**, which convert categorical variables into 0/1 variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dummy variables\n",
    "\n",
    "Let's take a look at the `Climate Change` variable to start out. This will be our outcome variable, or **label**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cah.group('Climate Change')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict the people who believe climate change is not real or not caused by people. This is two different categories, so we need to find a way to convert the `Climate Change` values into a variable with 0 if it was `DK/REF` or `Real and Caused by People`, and 1 if it was `Not Real At All` or `Real but not Caused by People`. We'll do this by creating a function and using `apply` (if you don't remember how to use `apply`, look back at Lab 4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_label(response):\n",
    "    '''\n",
    "    For a Climate Change response, turns it into a True or False depending on the answer.\n",
    "    \n",
    "    Arguments:\n",
    "    response: str, the response to the Climate Change questions.\n",
    "    \n",
    "    Returns:\n",
    "    A boolean\n",
    "    '''\n",
    "    \n",
    "    if response == 'Real but not Caused by People' :\n",
    "        return 1\n",
    "    if response == 'Not Real At All':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "climate_change_dummy = cah.apply(create_label, 'Climate Change')\n",
    "climate_change_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using `apply` to get an array of 0 and 1 values, we can add it back in to the Table and drop the original `Climate Change` variable. We'll call the new variable `label` since it is the label that we are trying to predict.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 1. Add the `climate_change_dummy` variable to the Table as a variable called `label`. Drop the `Climate Change` variable. Call the new Table `cah_dummy_label`.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Variables with Multiple Categories \n",
    "\n",
    "If we want to preserve the multiple categories, we can use multiple dummy variables for a given categorical variable. Let's take a look at the `Gender` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cah.group('Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this variable, we might want to make sure we keep each of the three unique categories. In order to do this, we can create two dummy variables that contain the same information as this one categorical variable. We'll create a variable called `Female` that is a 0 if the person is not Female and 1 if the person is Female. We'll also create a variable called `Male` that is 0 if the person is not Male and 1 if the person is Male. If a person responded \"Other\", then they will simply have a 0 on both `Female` and `Male`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "male = cah.column('Gender') == \"Male\"\n",
    "female = cah.column('Gender') == \"Female\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can add these back into the Table and drop the `Gender` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cah_dummy_gender_label = cah_dummy_label.with_columns('Male', male,\n",
    "                                                     'Female', female).drop('Gender')\n",
    "cah_dummy_gender_label.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at two other variables that we want to turn into dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cah.group('Political Affiliation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cah.group('Level of Education')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 2. Create dummy variables for all the other variables that are still categorical variables. Make sure to keep as much information as possible by creating multiple dummy variables if there are more than two categories. Leave out the `DK/REF` group for `Political Affiliation` and the `Other` group for `Level of Education`.**</font>\n",
    "\n",
    "Call the new Table with all of the dummy variables `cah_clean`. Make sure it does not have any of the original categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "democrat = ...\n",
    "republican = ...\n",
    "independent = ...\n",
    "\n",
    "college = ...\n",
    "graduate = ...\n",
    "high_school = ...\n",
    "some_college = ...\n",
    "\n",
    "cah_clean = ...\n",
    "cah_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create train and test sets\n",
    "\n",
    "First, let's split up the data into train and test sets. For this assignment, we will do a simple holdout set, assigning a random 20% of the data as the test data, and building the model on the remaining 80% of the data. \n",
    "\n",
    "<font color = 'red'>**Question 3. Create two Tables, one called `test` and one called `train`. The `test` table should contain a random 20% of the data, while the `train` Table should contain the other 80%.**<\\font>\n",
    "\n",
    "*Hint:* You can shuffle the entire dataset (sample the whole dataset without replacement), then just take the top 80% as your train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of rows you want to take by multiplying the number of rows in\n",
    "# movements by 0.8. Remember, this needs to be an integer!\n",
    "rows_to_take = ...\n",
    "\n",
    "# Shuffle the Table\n",
    "shuffled_cah = ...\n",
    "\n",
    "# Use .take and np.arange to split the data into train and test.\n",
    "# train should be the first rows_to_take rows of shuffled_cah\n",
    "# test should be the rest\n",
    "train = ...\n",
    "test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have 248 rows in the train set and 62 rows in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Fitting the models\n",
    "\n",
    "Now, let's use the `train` data to fit a Decision Tree model. We can do this using `DecisionTreeClassifier`, similar to how we used `KNeighborsClassifier`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "predictors = train.drop('label').rows\n",
    "outcome = train.column('label')\n",
    "\n",
    "tree.fit(X = predictors, y = outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `plot_tree` function to get an idea of what the decision making process looks like for this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_tree(tree)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very hard to read, and has lots of splits being made. It's likely that the tree is **overfitting** because we are making the model too close to our data. Let's try fitting a smaller tree model. We'll set the `max_depth` to 5, so that the tree only goes down 5 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth = 5)\n",
    "\n",
    "predictors = train.drop('label').rows\n",
    "outcome = train.column('label')\n",
    "\n",
    "tree.fit(X = predictors, y = outcome)\n",
    "plot_tree(tree)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is likely still too small to read, but that is ok. This should at least give you an idea of the size of the tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 4. Create three model objects called `tree2`, `tree3`, and `tree4` that represent the Decision Tree classifiers with max_depth = 2, 3, and 4, respectively. Using the `train` Table you created above, fit each of the three models.**</font>\n",
    "\n",
    "If you're not sure about the exact format of the data needed, remember that you need to use `.rows` for the `X` values and `.column` for the `y` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model objects\n",
    "tree2 = ...\n",
    "tree3 = ...\n",
    "tree4 = ...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Predict Test Set\n",
    "\n",
    "As before, we can use the `.predict_proba` method with the model objects to generate predict scores, and use those with a threshold to get our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a threshold \n",
    "threshold = 0.3\n",
    "\n",
    "# Make sure you fit the model before running this!\n",
    "test_features = test.drop('label').rows\n",
    "tree_predicted = tree.predict_proba(test_features)[:,1] > threshold\n",
    "tree_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `True` and `False` values correspond to a prediction of `1` (for `True`) and `0` (for `False`). When we created our `label` variable, we made sure to make the positive case of `1` to be believing Climate Change was not real or not caused by humans. So, we use `[:,1]` to make sure the `1` value is still our positive case. \n",
    "\n",
    "Since our `label` is already a 0/1 variable, we don't need to do anything to use it for our performance metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "expected = test.column('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 5. Create additional arrays that contain the predicted values for each of the models that we've fit (call them `tree2_predicted`, `tree3_predicted`, and `tree4_predicted`).**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2_predicted = ...\n",
    "tree3_predicted = ...\n",
    "tree4_predicted = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a confusion matrix using the `confusion_matrix` function that we brought in at the beginning. This is part of the `sklearn.metrics` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(expected,tree_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns represent predictions and the rows represent actual values, so the top left is **true negatives (TN)**, the bottom right is **true positives (TP)**, the top right is **false positives (FP)**, and the bottom left is **false negatives (FN)**.\n",
    "\n",
    "<img src=\"confusion_matrix.jpeg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluation \n",
    "\n",
    "Precision measures the accuracy of the classifier when it predicts an example to be positive. It is the ratio of correctly predicted positive examples to examples predicted to be positive. \n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "Recall measures the accuracy of the classifier to find positive examples in the data. \n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "By selecting different thresholds we can vary and tune the precision and recall of a given classifier. A conservative classifier (threshold 0.99) will classify a case as 1 only when it is *very sure*, leading to high precision. On the other end of the spectrum, a low threshold (e.g. 0.01) will lead to higher recall. \n",
    "\n",
    "We can use the `precision_score` and `recall_score` functions to find the value of these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(expected,tree_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(expected,tree_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 6. Find the confusion matrix for one of the other models. Use the `precision_score` and `recall_score` functions to find precision and recall for your models.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 7. Which model of two we measured the performance of above performed the best according to precision? Recall?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Repeating the steps\n",
    "\n",
    "We've done one iteration ... but we've only done it with one threshold, and we haven't tuned the parameters much. We won't go through all of the various ways we can fine-tune our models, but we can show how it is done: using loops.\n",
    "\n",
    "<font color = 'red'>**Question 8. Write a loop that tries thresholds of 0.1, 0.2, 0.3, 0.4, and 0.5 and max_depth values of 2, 3, 4, and 5 to make predictions using Decision Trees. Store all of the values tried within arrays so that they can be combined into a Table afterwards.**</font>\n",
    "\n",
    "The loop has been started below for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = make_array()\n",
    "depths = make_array()\n",
    "precisions = make_array()\n",
    "recalls = make_array()\n",
    "\n",
    "predictors = train.drop('label').rows\n",
    "outcome = train.column('label')\n",
    "\n",
    "expected = test.column('label')\n",
    "\n",
    "for threshold in make_array(0.1, 0.2, 0.3, 0.4, 0.5):\n",
    "    for depth in make_array(2, 3, 4, 5):\n",
    "        ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to look at the results afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use these to look at results\n",
    "tree_results = Table().with_columns('Threshold', thresholds,\n",
    "                                    'Max Depth', depths,\n",
    "                                    'Precision', precisions,\n",
    "                                    'Recall', recalls)\n",
    "tree_results.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 9. What model and threshold combination gives the best precision? The best recall? If there are ties, choose the one that has the better performance in the other metric (so, for example, if two model/threshold combinations are tied for best recall, choose the one with the better precision of the two. If they are tied in both, then just choose whichever one you want.).**</font>\n",
    "\n",
    "*Hint:* Use `.sort` to sort by a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Model Selection and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, when deciding on the best model, we compare the models we fit with each other, as well as against a baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'>**Question 10. Consider the threshold used in the best model by precision chosen in Question 9. How well does our best model perform compared to the baseline of a random model?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
